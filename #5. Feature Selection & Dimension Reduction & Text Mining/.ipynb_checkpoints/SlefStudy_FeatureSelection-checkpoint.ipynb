{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래 예제들을 실습해보면 같은 데이터를 사용하는데도 Select하는 피쳐들이 다른데 fit을 어떤함수에 하느냐에 따라서<br>Select하는 피쳐가 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Univariate Selection \n",
    "- Univariate : 단변량\n",
    "  - 단변량은 단 하나의 변수의 표현식, 방정식, 함수 또는 다항식을 나타냄. 하나의 변수\n",
    "  - Chi-sqd, t검정, 분산분석(ANOVA), 회귀분석(Regression) 등에 의한 자료분석은 단변량분석에 해당\n",
    "- Multivariate : 다변량\n",
    "  - 여러 개의 독립변수에 대한 여러 개의 종속변수를 동시에 분석해 보는 통계적 방법을 다변량분석\n",
    "  - 통계적으로는 종속변수의 관계성(relationships)을 고려한 상태에서 여러 개의 단변량분석을 동시에 수행하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
      " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
      " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
      " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
      " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "# url에 있는 data는 Feature가 없어서 names가 필요함.\n",
    "\n",
    "array = dataframe.values\n",
    "array2 = dataframe\n",
    "# print(\"어레이\")\n",
    "# print(array)\n",
    "# array2는 coulumn명이 들어가있는 데이터 프레임 형태이다.\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# X로 칭한 8개의 Column의 조합이 Y를 설명할 수 있다.\n",
    "print(X)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest(k=4, score_func=<function chi2 at 0x0000023AEF02BC80>)\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "# SelectKBest는 고정된 k개의 특성을 선택\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "# 위 내용은 카이스퀘어 검정으로 검정하여 4개의 피쳐를 뽑는다는 뜻\n",
    "\n",
    "fit = test.fit(X, Y)\n",
    "# X must contain only non-negative features such as booleans or frequencies\n",
    "# 카이스퀘어 분포는 항상 양수값만을 갖는 특징\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " ...\n",
      " [121.  112.   26.2  30. ]\n",
      " [126.    0.   30.1  47. ]\n",
      " [ 93.    0.   30.4  23. ]]\n"
     ]
    }
   ],
   "source": [
    "# summarize scores / 여기서 set_printoptions는 말그대로 print할 때 옵션을 설정해주는 것이고 precision이 3이라는 것은 소수 셋째자리까지 임.\n",
    "numpy.set_printoptions(precision=3)\n",
    "\n",
    "print(fit.scores_)\n",
    "#  [  111.52   @@1411.887@@    17.605    53.108  @@2175.565@@   @@127.669@@     5.393   @@181.304@@]\n",
    "#  2번째, 5번째, 6번째, 8번째 피쳐가 socre가 크니깐 선택됨을 알 수 있음.\n",
    "\n",
    "# fit.transform(X)해야 영향력이 큰 피쳐 4개를 뽑아줌\n",
    "features = fit.transform(X)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RFE(Recursive Feature Elimination) -> 재귀적 피쳐 소거\n",
    "- 재귀적으로 feature를 삭제해가면서 남아있는 feature들로 Model을 만든다.\n",
    "- 모델의 정확도로 target 피쳐를 예측하는데 가장 기여한 피쳐(혹은 피쳐들의 조합)을 식별함\n",
    "- 다음 예제는 RFE with the logistic regression algorithm to select the top 3 features. 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "# RFE(model, 3)에서 3은 피쳐 3개 고른다는 뜻\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: \" +str(fit.n_features_)) \n",
    "print(\"Selected Features: \" + str(fit.support_))\n",
    "print(\"Feature Ranking: \" +str(fit.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 4\n",
      "Selected Features: [ True  True False False False  True  True False]\n",
      "Feature Ranking: [1 1 2 4 5 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction2\n",
    "model = LogisticRegression()\n",
    "\n",
    "# SelectKBest한거랑 같은 결과가 나오는지 확인하기위해 뽑을 Feature를 4로 설정함\n",
    "rfe = RFE(model, 4)\n",
    "\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: \" +str(fit.n_features_)) \n",
    "print(\"Selected Features: \" + str(fit.support_))\n",
    "print(\"Feature Ranking: \" +str(fit.ranking_))\n",
    "\n",
    "# SelectKBest한거와 결과는 같지않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PCA(Principal Component Analysis) -> 주성분 분석\n",
    "- 주성분 분석 (또는 PCA)은 선형 대수를 사용하여 데이터 집합을 압축 된 형식으로 변환합니다.\n",
    "- 데이터 축소 기술 / 차원 축소 기술 이라고 불림.\n",
    "- PCA는 변환된 형태에서 차원 수 or 주성분 갯수 를 선택할 수 있음.\n",
    "- 아래 예제는 PCA and select 3 principal components이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with PCA\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can see that the transformed dataset (3 principal components) bare little resemblance to the source data.<br><br>해석 : 변환된 데이터 형태가 원래 데이터와 유사하지 않음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "# summarize components, 아래내용은 Explained Variance들의 합이 변수갯수를 3개로 줄였을때 몇%정확도로 설명할 수 있는지를 설명해주는 것.\n",
    "# 여기서 말하는 Explained Variance가 R^2인지 확실치않으니 다음 줄은 참고할 것.\n",
    "# Explained Variance는 결정계수(R^2) 이고 결정계수는 1에 가까울 수록 회귀식의 적합도가 높은 것임(잘 회귀 한다는 뜻.)\n",
    "print(\"Explained Variance: \" + str(fit.explained_variance_ratio_))\n",
    "\n",
    "# 이거는 왜 해야하는 건지 잘 모르겠다.\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Importance\n",
    "- RandomForest 나 Extra Trees에서 피쳐의 중요도를 평가할때 사용됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.096 0.218 0.095 0.08  0.078 0.167 0.125 0.142]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(model.feature_importances_)\n",
    "# print된 결과를 보고 2번째, 6번째, 8번째 feature가 중요도가 높음을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
